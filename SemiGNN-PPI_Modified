{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:01.498690Z","iopub.execute_input":"2024-12-20T21:09:01.498964Z","iopub.status.idle":"2024-12-20T21:09:07.479018Z","shell.execute_reply.started":"2024-12-20T21:09:01.498934Z","shell.execute_reply":"2024-12-20T21:09:07.478113Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, JumpingKnowledge, GCNConv, GAT\nfrom torch_geometric.datasets import PPI\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.utils import dense_to_sparse\nfrom torch_geometric.data import Data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:07.479974Z","iopub.execute_input":"2024-12-20T21:09:07.480226Z","iopub.status.idle":"2024-12-20T21:09:13.138579Z","shell.execute_reply.started":"2024-12-20T21:09:07.480206Z","shell.execute_reply":"2024-12-20T21:09:13.137618Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch_geometric.seed import seed_everything\n\nseed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.139374Z","iopub.execute_input":"2024-12-20T21:09:13.139737Z","iopub.status.idle":"2024-12-20T21:09:13.150013Z","shell.execute_reply.started":"2024-12-20T21:09:13.139716Z","shell.execute_reply":"2024-12-20T21:09:13.149164Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# -------------------------\n# Label Learner Component\n# -------------------------\nclass LabelGraphLearner(nn.Module):\n    def __init__(self, num_labels):\n        super(LabelGraphLearner, self).__init__()\n        self.label_adj = nn.Parameter(torch.randn(num_labels, num_labels))  # Learnable adjacency matrix\n\n    def forward(self):\n        \"\"\"\n        Learn the label adjacency matrix as edge probabilities.\n        :return: Learned label adjacency matrix (softmax-normalized).\n        \"\"\"\n        adj = torch.sigmoid(self.label_adj)  # Map to [0, 1] as edge probabilities\n        return adj\n\nclass LabelGCN(nn.Module):\n    def __init__(self, num_labels, hidden_channels):\n        super(LabelGCN, self).__init__()\n        self.conv1 = GCNConv(num_labels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n\n    def forward(self, label_adj):\n        \"\"\"\n        Propagate through the label graph using GCN.\n        :param label_adj: Adjacency matrix for labels.\n        :return: Refined label embeddings.\n        \"\"\"\n        x = torch.eye(label_adj.size(0)).to(label_adj.device)  # One-hot encoding for labels\n        edge_index, _ = dense_to_sparse(label_adj)  # Convert adjacency matrix to edge_index format\n        x = F.relu(self.conv1(x, edge_index))\n        x = self.conv2(x, edge_index)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.150795Z","iopub.execute_input":"2024-12-20T21:09:13.151041Z","iopub.status.idle":"2024-12-20T21:09:13.161709Z","shell.execute_reply.started":"2024-12-20T21:09:13.151020Z","shell.execute_reply":"2024-12-20T21:09:13.160842Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# -------------------------\n# Graph Augmentation\n# -------------------------\n\ndef edge_manipulation(edge_index, num_nodes, ratio=0.1):\n    \"\"\"\n    Perform edge manipulation by randomly replacing a percentage of edges.\n    :param edge_index: Original edge index (2 x num_edges).\n    :param num_nodes: Number of nodes in the graph.\n    :param ratio: Percentage of edges to replace.\n    :return: Modified edge index.\n    \"\"\"\n    device = edge_index.device  # Ensure all tensors are created on the same device\n    num_edges = edge_index.size(1)\n    num_replacements = int(ratio * num_edges)\n\n    # Randomly select edges to replace\n    replacement_indices = torch.randint(0, num_edges, (num_replacements,), device=device)\n    new_edges = torch.randint(0, num_nodes, (2, num_replacements), device=device)  # Random new edges\n\n    # Replace selected edges\n    modified_edge_index = edge_index.clone()  # Clone to avoid modifying the original tensor\n    modified_edge_index[:, replacement_indices] = new_edges\n    return modified_edge_index\n\n\ndef node_manipulation(x, ratio=0.1):\n    \"\"\"\n    Perform node manipulation by randomly masking a percentage of node features.\n    :param x: Node features (num_nodes x feature_dim).\n    :param ratio: Percentage of node features to mask.\n    :return: Modified node features.\n    \"\"\"\n    num_nodes, feature_dim = x.size()\n    num_masked = int(ratio * num_nodes)\n\n    # Randomly select nodes to mask\n    mask_indices = torch.randint(0, num_nodes, (num_masked,))\n    x[mask_indices] = 0  # Mask selected nodes\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.162560Z","iopub.execute_input":"2024-12-20T21:09:13.162787Z","iopub.status.idle":"2024-12-20T21:09:13.177777Z","shell.execute_reply.started":"2024-12-20T21:09:13.162768Z","shell.execute_reply":"2024-12-20T21:09:13.177034Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# -------------------------\n# Student Model\n# -------------------------\nclass StudentModel(nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, use_jk=False):\n        super(StudentModel, self).__init__()\n        self.use_jk = use_jk\n\n        # GIN Layers for node embeddings\n        self.gin_convs = nn.ModuleList()\n        self.gin_convs.append(\n            GINConv(nn.Sequential(\n                nn.Linear(in_channels, hidden_channels),\n                nn.ReLU(),\n                nn.Linear(hidden_channels, hidden_channels),\n                nn.ReLU(),\n                nn.BatchNorm1d(hidden_channels)\n            ))\n        )\n        for _ in range(num_layers - 1):\n            self.gin_convs.append(\n                GINConv(nn.Sequential(\n                    nn.Linear(hidden_channels, hidden_channels),\n                    nn.ReLU(),\n                    nn.Linear(hidden_channels, hidden_channels),\n                    nn.ReLU(),\n                    nn.BatchNorm1d(hidden_channels)\n                ))\n            )\n\n        if self.use_jk:\n            self.jk = JumpingKnowledge(mode='cat')\n            self.lin1 = nn.Linear(num_layers * hidden_channels, hidden_channels)\n        else:\n            self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n\n        # Label learning components\n        self.label_learner = LabelGraphLearner(out_channels)\n        self.label_gcn = LabelGCN(out_channels, hidden_channels)\n\n        # Graph-based interaction layer\n        self.node_label_interaction = GAT(hidden_channels, out_channels, num_layers=1)\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        Forward pass for the student model.\n        :param x: Node features.\n        :param edge_index: Graph structure.\n        :return: Node-label logits and label adjacency matrix.\n        \"\"\"\n        # Node embeddings\n        embeddings = []\n        for conv in self.gin_convs:\n            x = conv(x, edge_index)\n            embeddings.append(x)\n\n        if self.use_jk:\n            x = self.jk(embeddings)\n        x = F.relu(self.lin1(x))  # Node embeddings (shape: [num_nodes, hidden_channels])\n\n        # Label embeddings\n        label_adj = self.label_learner()  # Learn the label graph\n        label_embeddings = self.label_gcn(label_adj)  # Refine label embeddings (shape: [num_labels, hidden_channels])\n\n        # Combine node and label embeddings into a joint graph\n        # Create a node-label interaction graph\n        num_nodes = x.size(0)\n        num_labels = label_embeddings.size(0)\n\n        # Concatenate node and label embeddings\n        combined_embeddings = torch.cat([x, label_embeddings], dim=0)  # Shape: [num_nodes + num_labels, hidden_channels]\n\n        # Create interaction edges between all nodes and labels\n        node_indices = torch.arange(num_nodes)\n        label_indices = torch.arange(num_labels) + num_nodes\n        interaction_edges = torch.stack(torch.meshgrid(node_indices, label_indices)).reshape(2, -1).to(edge_index.device)\n\n        # Pass through GCN for node-label interaction\n        interaction_logits = self.node_label_interaction(combined_embeddings, interaction_edges)  # Shape: [num_nodes + num_labels, out_channels]\n\n        # Extract node-label logits\n        node_label_logits = interaction_logits[:num_nodes]  # Shape: [num_nodes, num_labels]\n\n        return node_label_logits, x, label_adj\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.179493Z","iopub.execute_input":"2024-12-20T21:09:13.179742Z","iopub.status.idle":"2024-12-20T21:09:13.198357Z","shell.execute_reply.started":"2024-12-20T21:09:13.179708Z","shell.execute_reply":"2024-12-20T21:09:13.197645Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# -------------------------\n# Teacher Model\n# -------------------------\nclass TeacherModel(nn.Module):\n    def __init__(self, student_model):\n        super(TeacherModel, self).__init__()\n        # Clone student model structure\n        self.model = StudentModel(\n            in_channels=student_model.gin_convs[0].nn[0].in_features,\n            hidden_channels=student_model.gin_convs[0].nn[0].out_features,\n            out_channels=student_model.label_learner.label_adj.shape[0],\n            num_layers=len(student_model.gin_convs),\n            use_jk=student_model.use_jk\n        )\n        self.model.load_state_dict(student_model.state_dict())  # Initialize with student weights\n\n    @torch.no_grad()\n    def update(self, student_model, alpha=0.5):\n        \"\"\"Update teacher parameters using EMA.\"\"\"\n        for teacher_param, student_param in zip(self.model.parameters(), student_model.parameters()):\n            teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data\n\n    def forward(self, x, edge_index):\n        return self.model(x, edge_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.199197Z","iopub.execute_input":"2024-12-20T21:09:13.199429Z","iopub.status.idle":"2024-12-20T21:09:13.221394Z","shell.execute_reply.started":"2024-12-20T21:09:13.199409Z","shell.execute_reply":"2024-12-20T21:09:13.220562Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# -------------------------\n# Semi-Supervised Loss Function\n# -------------------------\n\ndef supervised_loss(student_out, labels, labeled_mask):\n    \"\"\"\n    Supervised loss for labeled data using Binary Cross-Entropy with logits.\n    \"\"\"\n    bce_loss = F.binary_cross_entropy_with_logits(\n        student_out[labeled_mask], labels[labeled_mask]\n    )\n    return bce_loss\n\n\ndef consistency_loss(student_out, teacher_out, unlabeled_mask):\n    \"\"\"\n    Calculate consistency loss for unlabeled nodes.\n    :param student_out: Output logits from the student model.\n    :param teacher_out: Output logits from the teacher model.\n    :param unlabeled_mask: Mask identifying unlabeled nodes.\n    :return: Consistency loss value.\n    \"\"\"\n    # Apply sigmoid to logits to get probabilities\n    student_probs = torch.sigmoid(student_out[unlabeled_mask])\n    teacher_probs = torch.sigmoid(teacher_out[unlabeled_mask])\n\n    # Compute the L2 loss between student and teacher predictions\n    return F.mse_loss(student_probs, teacher_probs)\n\ndef calculate_adjacency_matrix(embeddings):\n    \"\"\"\n    Compute the adjacency matrix using Pearson's Correlation Coefficient (PCC).\n    :param embeddings: Node embeddings (tensor of shape [num_nodes, hidden_dim]).\n    :return: Adjacency matrix (tensor of shape [num_nodes, num_nodes]).\n    \"\"\"\n    embeddings = embeddings - embeddings.mean(dim=0, keepdim=True)\n    norm = embeddings.norm(dim=1, keepdim=True)\n    normalized_embeddings = embeddings / (norm + 1e-8)  # Prevent division by zero\n    adj_matrix = torch.mm(normalized_embeddings, normalized_embeddings.T)\n    return adj_matrix\n\ndef edge_matching_loss(student_embeddings, teacher_embeddings):\n    \"\"\"\n    Compute the edge loss using adjacency matrices.\n    :param student_embeddings: Student node embeddings (tensor of shape [num_nodes, hidden_dim]).\n    :param teacher_embeddings: Teacher node embeddings (tensor of shape [num_nodes, hidden_dim]).\n    :return: Edge loss (scalar).\n    \"\"\"\n    adj_student = calculate_adjacency_matrix(student_embeddings)\n    adj_teacher = calculate_adjacency_matrix(teacher_embeddings)\n    return F.mse_loss(adj_student, adj_teacher)\n\n\ndef node_matching_loss(student_embeddings, teacher_embeddings):\n    \"\"\"\n    Compute the node loss by aligning diagonal elements of the cross-embedding adjacency matrix.\n    :param student_embeddings: Student node embeddings (tensor of shape [num_nodes, hidden_dim]).\n    :param teacher_embeddings: Teacher node embeddings (tensor of shape [num_nodes, hidden_dim]).\n    :return: Node loss (scalar).\n    \"\"\"\n    # Cross-embedding adjacency matrix\n    student_normalized = student_embeddings / (student_embeddings.norm(dim=1, keepdim=True) + 1e-8)\n    teacher_normalized = teacher_embeddings / (teacher_embeddings.norm(dim=1, keepdim=True) + 1e-8)\n    cross_adj_matrix = torch.mm(student_normalized, teacher_normalized.T)\n\n    # Diagonal elements should align with identity\n    diagonal_elements = torch.diagonal(cross_adj_matrix)\n    identity = torch.ones_like(diagonal_elements)\n    return F.mse_loss(diagonal_elements, identity)\n\ndef create_label_adjacency(data_y):\n    \"\"\"\n    Create an adjacency matrix based on shared labels in data.y.\n    \n    :param data_y: Tensor of shape [num_nodes, num_labels], where each entry is 1 if the node has the label, 0 otherwise.\n    :return: Adjacency matrix (Tensor of shape [num_labels, num_labels]).\n    \"\"\"\n    # Compute the co-occurrence matrix\n    co_occurrence = torch.matmul(data_y.T, data_y)  # Shape: [num_labels, num_labels]\n    \n    # Convert co-occurrence to adjacency (binary)\n    adjacency = (co_occurrence > 0).float()  # Shape: [num_labels, num_labels]\n    \n    # Remove self-loops\n    adjacency.fill_diagonal_(0)\n    \n    return adjacency\n\n\ndef label_adj_loss(predicted_adj, true_adj):\n    return F.mse_loss(predicted_adj, true_adj)\n\ndef semignn_loss(\n    student_out, teacher_out, labels, labeled_mask, unlabeled_mask,\n    student_embeddings, teacher_embeddings, label_adj,\n    lambda_con=0.1, lambda_edge=0.1, lambda_node=0.1, lambda_label=0.1\n):\n    \"\"\"\n    Combined loss function for SemiGNN-PPI.\n    \"\"\"\n    # Compute individual loss components\n    sup_loss = supervised_loss(student_out, labels, labeled_mask)\n    con_loss = consistency_loss(student_out, teacher_out, unlabeled_mask)\n    edge_loss = edge_matching_loss(student_embeddings, teacher_embeddings)\n    node_loss = node_matching_loss(student_embeddings, teacher_embeddings)\n\n    # Label adjacency loss\n    true_adj = create_label_adjacency(labels)\n    adj_loss = label_adj_loss(label_adj, true_adj)\n    \n    # Combine losses with scaling factors\n    total_loss = (\n        sup_loss\n        + lambda_con * con_loss\n        + lambda_edge * edge_loss\n        + lambda_node * node_loss\n        + lambda_label * adj_loss\n    )\n    return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.222415Z","iopub.execute_input":"2024-12-20T21:09:13.222771Z","iopub.status.idle":"2024-12-20T21:09:13.243203Z","shell.execute_reply.started":"2024-12-20T21:09:13.222735Z","shell.execute_reply":"2024-12-20T21:09:13.242426Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------------\n# Training Framework\n# -------------------------\nclass SemiGNNFrameworkPPI:\n    def __init__(self, student_model, teacher_model, device):\n        self.student = student_model.to(device)\n        self.teacher = teacher_model.to(device)\n        self.device = device\n        self.optimizer = torch.optim.Adam(self.student.parameters(), lr=0.001)\n        self.lambda_con = 0.02\n\n    def train_step(self, loader):\n        self.student.train()\n        self.teacher.eval()\n        total_loss = 0\n\n        for data in loader:\n            data = data.to(self.device)\n            labeled_mask = data.train_mask\n            unlabeled_mask = ~data.train_mask\n\n            # Apply edge manipulation\n            edge_index_student = edge_manipulation(data.edge_index, data.x.size(0), ratio=0.1)  # 10% for student\n            edge_index_teacher = edge_manipulation(data.edge_index, data.x.size(0), ratio=0.05)  # 5% for teacher\n    \n            # Apply node manipulation\n            x_student = node_manipulation(data.x.clone(), ratio=0.1)  # 10% for student\n            x_teacher = node_manipulation(data.x.clone(), ratio=0.05)  # 5% for teacher\n\n            # Student forward pass\n            student_out, student_embeddings, label_adj =self.student(x_student, edge_index_student)\n\n            # Teacher forward pass\n            with torch.no_grad():\n                teacher_out, teacher_embeddings, _ = self.teacher(x_teacher, edge_index_teacher)\n\n            # Compute loss\n            loss = semignn_loss(\n                student_out, teacher_out, data.y, labeled_mask, unlabeled_mask,\n                student_embeddings, teacher_embeddings, label_adj,\n                lambda_con=0.02, lambda_edge=0.01, lambda_node=0.003, lambda_label=1  # Loss scaling factors based on SEMIGNN-PPI paper\n            )\n\n            # Backpropagation\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            total_loss += loss.item()\n\n        # Update teacher model\n        self.teacher.update(self.student)\n        return total_loss / len(loader)\n\n\n    def evaluate(self, loader):\n      \"\"\"Evaluate the student model for multi-label node classification.\"\"\"\n      self.student.eval()\n      #total_correct, total_labels = 0, 0\n      true_positives = 0\n      false_positives = 0\n      false_negatives = 0\n\n      for data in loader:\n          data = data.to(self.device)\n          with torch.no_grad():\n              out, _, _ = self.student(data.x, data.edge_index)  # Raw logits\n              prob = torch.sigmoid(out)                   # Probabilities (0 to 1)\n              pred = (prob > 0.5).long()                  # Binarize predictions\n              # print(data.val_mask.shape)\n              # print(pred.shape)\n              # print(data.y.shape)\n\n              # Validation mask and multi-label accuracy\n              #correct = (pred == data.y).sum().item()\n              #total_correct += correct\n              #total_labels += data.y.size(0) * data.y.size(1)  # Total labels (nodes * 121)\n              true_positives += ((pred == 1) & (data.y == 1)).sum().item()  # TP: Correctly predicted 1s\n              false_positives += ((pred == 1) & (data.y == 0)).sum().item()  # FP: Predicted 1, actual 0\n              false_negatives += ((pred == 0) & (data.y == 1)).sum().item()  # FN: Predicted 0, actual 1\n            \n      # Compute precision, recall, and F1-score\n      precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n      recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n      f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n      #accuracy = total_correct / total_labels\n      return f1_score #accuracy\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.244418Z","iopub.execute_input":"2024-12-20T21:09:13.244785Z","iopub.status.idle":"2024-12-20T21:09:13.262079Z","shell.execute_reply.started":"2024-12-20T21:09:13.244749Z","shell.execute_reply":"2024-12-20T21:09:13.261268Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# -------------------------\n# Main Script\n# -------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# Load PPI dataset\ntrain_dataset = PPI(root='data/PPI', split='train')\nval_dataset = PPI(root='data/PPI', split='val')\ntest_dataset = PPI(root='data/PPI', split='test')\n\n\ndef create_train_mask(data, mask_ratio=0.9):\n    \"\"\"\n    Create a train_mask for nodes in a graph.\n    :param data: A graph in the PPI dataset (torch_geometric.data.Data).\n    :param train_ratio: Fraction of nodes to use for training.\n    :return: train_mask (torch.Tensor)\n    \"\"\"\n    num_nodes = data.x.size(0)  # Total number of nodes\n    num_mask = int(mask_ratio * num_nodes)  # Number of training nodes\n\n    # Randomly permute indices and select train nodes\n    perm = torch.randperm(num_nodes)\n    mask_indices = perm[:num_mask]\n\n    # Initialize train_mask\n    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    train_mask[mask_indices] = True\n    return train_mask\n\n# Update train_dataset with train_mask\nupdated_train_dataset = []\nfor graph in train_dataset:\n    train_mask = create_train_mask(graph, mask_ratio=0.05)\n    updated_graph = Data(\n        x=graph.x,\n        edge_index=graph.edge_index,\n        y=graph.y,\n        train_mask=train_mask\n    )\n    updated_train_dataset.append(updated_graph)\n\n\ntrain_loader = DataLoader(updated_train_dataset, batch_size=2, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n\n# Initialize models\nstudent_model = StudentModel(in_channels=train_dataset.num_features, hidden_channels=512,\n                             out_channels=train_dataset.num_classes, num_layers=3, use_jk=True)\nteacher_model = TeacherModel(student_model)\n\n# Initialize framework\nframework = SemiGNNFrameworkPPI(student_model, teacher_model, device)\n\n# Training loop\nfor epoch in range(100):\n    train_loss = framework.train_step(train_loader)\n    val_accuracy = framework.evaluate(val_loader)\n    print(f\"Epoch {epoch+1:02d}: Train Loss: {train_loss: .4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n# Testing\ntest_accuracy = framework.evaluate(test_loader)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:09:13.263033Z","iopub.execute_input":"2024-12-20T21:09:13.263333Z","iopub.status.idle":"2024-12-20T21:11:14.386363Z","shell.execute_reply.started":"2024-12-20T21:09:13.263312Z","shell.execute_reply":"2024-12-20T21:11:14.385215Z"}},"outputs":[{"name":"stderr","text":"Downloading https://data.dgl.ai/dataset/ppi.zip\nExtracting data/PPI/ppi.zip\nProcessing...\nDone!\n/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: Train Loss:  0.8927, Validation Accuracy: 0.4767\nEpoch 02: Train Loss:  0.8195, Validation Accuracy: 0.5374\nEpoch 03: Train Loss:  0.7831, Validation Accuracy: 0.5200\nEpoch 04: Train Loss:  0.7668, Validation Accuracy: 0.5559\nEpoch 05: Train Loss:  0.7394, Validation Accuracy: 0.5802\nEpoch 06: Train Loss:  0.7166, Validation Accuracy: 0.5832\nEpoch 07: Train Loss:  0.7045, Validation Accuracy: 0.5796\nEpoch 08: Train Loss:  0.6826, Validation Accuracy: 0.6064\nEpoch 09: Train Loss:  0.6649, Validation Accuracy: 0.6073\nEpoch 10: Train Loss:  0.6532, Validation Accuracy: 0.5944\nEpoch 11: Train Loss:  0.6342, Validation Accuracy: 0.6230\nEpoch 12: Train Loss:  0.6133, Validation Accuracy: 0.6175\nEpoch 13: Train Loss:  0.6097, Validation Accuracy: 0.6246\nEpoch 14: Train Loss:  0.5985, Validation Accuracy: 0.6067\nEpoch 15: Train Loss:  0.5822, Validation Accuracy: 0.6279\nEpoch 16: Train Loss:  0.5700, Validation Accuracy: 0.6356\nEpoch 17: Train Loss:  0.5533, Validation Accuracy: 0.6506\nEpoch 18: Train Loss:  0.5472, Validation Accuracy: 0.6458\nEpoch 19: Train Loss:  0.5343, Validation Accuracy: 0.6347\nEpoch 20: Train Loss:  0.5306, Validation Accuracy: 0.6533\nEpoch 21: Train Loss:  0.5188, Validation Accuracy: 0.6594\nEpoch 22: Train Loss:  0.5130, Validation Accuracy: 0.6559\nEpoch 23: Train Loss:  0.5060, Validation Accuracy: 0.6612\nEpoch 24: Train Loss:  0.4895, Validation Accuracy: 0.6666\nEpoch 25: Train Loss:  0.4790, Validation Accuracy: 0.6780\nEpoch 26: Train Loss:  0.4720, Validation Accuracy: 0.6762\nEpoch 27: Train Loss:  0.4671, Validation Accuracy: 0.6784\nEpoch 28: Train Loss:  0.4539, Validation Accuracy: 0.6747\nEpoch 29: Train Loss:  0.4500, Validation Accuracy: 0.6829\nEpoch 30: Train Loss:  0.4400, Validation Accuracy: 0.6854\nEpoch 31: Train Loss:  0.4357, Validation Accuracy: 0.6858\nEpoch 32: Train Loss:  0.4252, Validation Accuracy: 0.6712\nEpoch 33: Train Loss:  0.4246, Validation Accuracy: 0.6906\nEpoch 34: Train Loss:  0.4185, Validation Accuracy: 0.6921\nEpoch 35: Train Loss:  0.4141, Validation Accuracy: 0.6949\nEpoch 36: Train Loss:  0.3995, Validation Accuracy: 0.7001\nEpoch 37: Train Loss:  0.3944, Validation Accuracy: 0.7023\nEpoch 38: Train Loss:  0.3937, Validation Accuracy: 0.7049\nEpoch 39: Train Loss:  0.3872, Validation Accuracy: 0.6952\nEpoch 40: Train Loss:  0.3834, Validation Accuracy: 0.6855\nEpoch 41: Train Loss:  0.3828, Validation Accuracy: 0.6968\nEpoch 42: Train Loss:  0.3776, Validation Accuracy: 0.6965\nEpoch 43: Train Loss:  0.3715, Validation Accuracy: 0.7049\nEpoch 44: Train Loss:  0.3702, Validation Accuracy: 0.7085\nEpoch 45: Train Loss:  0.3615, Validation Accuracy: 0.7052\nEpoch 46: Train Loss:  0.3589, Validation Accuracy: 0.7090\nEpoch 47: Train Loss:  0.3546, Validation Accuracy: 0.7105\nEpoch 48: Train Loss:  0.3445, Validation Accuracy: 0.7001\nEpoch 49: Train Loss:  0.3415, Validation Accuracy: 0.7065\nEpoch 50: Train Loss:  0.3414, Validation Accuracy: 0.7098\nEpoch 51: Train Loss:  0.3386, Validation Accuracy: 0.7150\nEpoch 52: Train Loss:  0.3375, Validation Accuracy: 0.7051\nEpoch 53: Train Loss:  0.3310, Validation Accuracy: 0.7104\nEpoch 54: Train Loss:  0.3296, Validation Accuracy: 0.7129\nEpoch 55: Train Loss:  0.3207, Validation Accuracy: 0.7103\nEpoch 56: Train Loss:  0.3155, Validation Accuracy: 0.7149\nEpoch 57: Train Loss:  0.3165, Validation Accuracy: 0.7199\nEpoch 58: Train Loss:  0.3132, Validation Accuracy: 0.7194\nEpoch 59: Train Loss:  0.3094, Validation Accuracy: 0.7160\nEpoch 60: Train Loss:  0.2998, Validation Accuracy: 0.7181\nEpoch 61: Train Loss:  0.2998, Validation Accuracy: 0.7178\nEpoch 62: Train Loss:  0.2968, Validation Accuracy: 0.7211\nEpoch 63: Train Loss:  0.2930, Validation Accuracy: 0.7192\nEpoch 64: Train Loss:  0.2932, Validation Accuracy: 0.7182\nEpoch 65: Train Loss:  0.2884, Validation Accuracy: 0.7253\nEpoch 66: Train Loss:  0.2854, Validation Accuracy: 0.7232\nEpoch 67: Train Loss:  0.2805, Validation Accuracy: 0.7187\nEpoch 68: Train Loss:  0.2859, Validation Accuracy: 0.7210\nEpoch 69: Train Loss:  0.2809, Validation Accuracy: 0.7218\nEpoch 70: Train Loss:  0.2749, Validation Accuracy: 0.7240\nEpoch 71: Train Loss:  0.2738, Validation Accuracy: 0.7261\nEpoch 72: Train Loss:  0.2683, Validation Accuracy: 0.7285\nEpoch 73: Train Loss:  0.2661, Validation Accuracy: 0.7240\nEpoch 74: Train Loss:  0.2580, Validation Accuracy: 0.7287\nEpoch 75: Train Loss:  0.2564, Validation Accuracy: 0.7265\nEpoch 76: Train Loss:  0.2573, Validation Accuracy: 0.7248\nEpoch 77: Train Loss:  0.2523, Validation Accuracy: 0.7316\nEpoch 78: Train Loss:  0.2486, Validation Accuracy: 0.7286\nEpoch 79: Train Loss:  0.2473, Validation Accuracy: 0.7307\nEpoch 80: Train Loss:  0.2406, Validation Accuracy: 0.7305\nEpoch 81: Train Loss:  0.2424, Validation Accuracy: 0.7224\nEpoch 82: Train Loss:  0.2421, Validation Accuracy: 0.7283\nEpoch 83: Train Loss:  0.2457, Validation Accuracy: 0.7274\nEpoch 84: Train Loss:  0.2393, Validation Accuracy: 0.7234\nEpoch 85: Train Loss:  0.2364, Validation Accuracy: 0.7287\nEpoch 86: Train Loss:  0.2298, Validation Accuracy: 0.7310\nEpoch 87: Train Loss:  0.2286, Validation Accuracy: 0.7267\nEpoch 88: Train Loss:  0.2304, Validation Accuracy: 0.7213\nEpoch 89: Train Loss:  0.2347, Validation Accuracy: 0.7196\nEpoch 90: Train Loss:  0.2336, Validation Accuracy: 0.7312\nEpoch 91: Train Loss:  0.2305, Validation Accuracy: 0.7297\nEpoch 92: Train Loss:  0.2265, Validation Accuracy: 0.7328\nEpoch 93: Train Loss:  0.2217, Validation Accuracy: 0.7299\nEpoch 94: Train Loss:  0.2226, Validation Accuracy: 0.7318\nEpoch 95: Train Loss:  0.2190, Validation Accuracy: 0.7300\nEpoch 96: Train Loss:  0.2162, Validation Accuracy: 0.7298\nEpoch 97: Train Loss:  0.2181, Validation Accuracy: 0.7335\nEpoch 98: Train Loss:  0.2121, Validation Accuracy: 0.7364\nEpoch 99: Train Loss:  0.2079, Validation Accuracy: 0.7346\nEpoch 100: Train Loss:  0.2060, Validation Accuracy: 0.7329\nTest Accuracy: 0.7511\n","output_type":"stream"}],"execution_count":10}]}